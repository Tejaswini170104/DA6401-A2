{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTVyOR9Q070J"
   },
   "source": [
    "**Question 01**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri6JVzTdIMAN"
   },
   "source": [
    "### (a) Adjusting Input Image Dimensions for Pretrained Models\n",
    "\n",
    "- **ImageNet pretrained models** (e.g., ResNet, VGG, EfficientNet) expect input images to have a resolution of **224×224 pixels**.\n",
    "- **iNaturalist dataset images** may have different sizes and aspect ratios.\n",
    "- To ensure compatibility with the pretrained model:\n",
    "  - **Resize** all images to match the input size expected by the model.\n",
    "  - Use the `transforms.Resize()` function in PyTorch to resize images.\n",
    "- Example of resizing:\n",
    "\n",
    "  ```python\n",
    "  transforms.Resize((224, 224))  # Resize to match input size expected by most pretrained models\n",
    "  ```\n",
    "\n",
    "\n",
    "## (b) Extending the Output Layer for a 10-Class Classification Task\n",
    "\n",
    "\n",
    "- Instead of replacing the last fully connected (FC) layer, we **append** a new output layer after the existing 1000-class layer.\n",
    "- This allows the model to preserve the learned 1000-class representation and apply an additional transformation for the iNaturalist task.\n",
    "\n",
    "Example modification for ResNet50:\n",
    "\n",
    "```python\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the pretrained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Extract the original 1000-class fully connected layer\n",
    "original_fc = model.fc\n",
    "\n",
    "# Create a new sequential head\n",
    "model.fc = nn.Sequential(\n",
    "    original_fc,            # Existing 1000-class output\n",
    "    nn.ReLU(),              # Optional non-linearity\n",
    "    nn.Linear(1000, 10)     # New output layer for 10-class classification\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQJER4jBIioG"
   },
   "source": [
    "**Question 02**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT1cWEbJJbda"
   },
   "source": [
    "### Strategies to Make Fine-Tuning Tractable\n",
    "\n",
    "1. **Freezing All Layers Except the Last Layer**:\n",
    "  - In this approach, all the layers of the model are frozen, meaning their weights remain unchanged during training. Only the final fully connected (FC) layer is trainable, allowing the model to adapt to the new task without requiring full retraining.\n",
    "  - This approach leverages the pretrained features in the earlier layers, which are generally useful for a wide range of tasks, while only training the final layer to match the specific 10-class task.\n",
    "\n",
    "  Example:\n",
    "  ```python\n",
    "  for param in model.parameters():\n",
    "       param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "  # Only the last layer will be updated\n",
    "  model.fc.requires_grad = True  # Unfreeze the last FC layer\n",
    "  ```\n",
    "\n",
    "2. **Freezing Layers Up to the k-th Layer**:\n",
    "  - In this strategy, the first few layers are frozen, and only the layers after a certain point (e.g., the k-th layer) are allowed to be updated. The idea here is to freeze the early layers which learn basic features (like edges and textures) that are likely transferable across different tasks, while fine-tuning the later layers to capture task-specific details.\n",
    "  - This approach is a middle ground between freezing all layers and training the entire model.\n",
    "\n",
    "  Example:\n",
    "  ```python\n",
    "  for i, param in enumerate(model.parameters()):\n",
    "      if i < k:\n",
    "          param.requires_grad = False  # Freeze layers up to k-th layer\n",
    "      else:\n",
    "          param.requires_grad = True  # Unfreeze layers after k-th layer\n",
    "  ```\n",
    "\n",
    "3. **Fine-Tuning Only the Last Few Layers**:\n",
    "  - Another approach is to **fine-tune only the last few layers**, such as the final few convolutional or dense layers. The idea behind this strategy is to allow the model to adapt to the specific characteristics of the new dataset while retaining the powerful features learned by the earlier layers.\n",
    "  - In this case, the number of layers to fine-tune can be adjusted based on computational constraints and the task's complexity.\n",
    "\n",
    "  Example:\n",
    "  ```python\n",
    "  # Freezing early layers and fine-tuning the last few layers\n",
    "  for param in model.parameters():\n",
    "      param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "  # Unfreeze the last 2 layers\n",
    "  for param in model.layer4.parameters():\n",
    "      param.requires_grad = True\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d62e9ba804a540f0b1dd04ec0d572fbc",
      "392a03d3f0274575abe197134c228ef5",
      "04312d93865a492a928138aa077d5aea",
      "ef8a6d865e93444eab1c2197a23df352",
      "d2f7c354f1c442a9b9d433ae2363f3da",
      "1448e5fdea4d406ca56906efcc19acfa",
      "9ff6748f55fb4137ae86b2f9ea551ad6",
      "6ffa291caecd4f60865b9540ff93dd75",
      "33e3b2cdd82c43ee91acbcd36c7aafa3",
      "100ecd065aa043fabc33b898f53634c9",
      "767909fce2db4350abda725bd616a270",
      "3db2ec5887ce499a89f5550e5e561a08",
      "18c1f400a1c042fab07662015cfa9504",
      "402e3bae1e514e02a3a86a8fedf31237",
      "ae0d54ba651642f793ae4592f3668264",
      "202928fa764d4414a1b045862ca4cc89",
      "c7eb3afd6cc24b019d9b49d17ac764bb",
      "99c603e4900549e79b5fe10e67853e4b",
      "71966033da0c44db8640eb1ae7bd79ac",
      "7eaf3cdc432b4f8bacd1c4ac893455c4",
      "1da77702b33441b4bcf3caf7ccbf410b",
      "a719aad4015740b2ba25787f08b5d706",
      "5c8ffc8bb36f4a129939176a0960990e",
      "7aa15d94874745b8adeec1716ffbb40d",
      "be6e9c10e5fc4fecbc5b69bf43917d49",
      "64526f9531a147b889d4d7f92781ca9a",
      "72c58bfe016742b3848c88d2b6447454",
      "b8d9573686f84fa88530821f6a1a95d1",
      "8b64beb706784d0eae7d130c4b5829c5",
      "38621f7f638946829e6a9a2a0ca6ba5f",
      "de378553cc074d88a17a9bed6b9b5435",
      "e060e8325b624924a78b3a38e2765993",
      "2a72afce17c14f68a8f1026431bcf3e4",
      "4328c00841df42e5828e4df081bd9bf4",
      "a499df43b0724e3a921115b3524d2f7e",
      "1dc245e5d72e4381952e8b0521772d0a",
      "0182542aee3e459986ac1656942cf5ff",
      "3e456022f6e34781a40decb08a863c84",
      "7ec05aacb1c84620a4f1a6406340e83d",
      "ab61f94c720a4a98a34a86b7b6899e34",
      "bf4d322d806a4555bb5a107c5729f629",
      "509a2380a85c44a8b62f476ce9400b16",
      "fe25f7cbead84dcd9fe0b2a5aa5eeb5a",
      "7f6c954b526d473185e512e9ec09ea38",
      "a927e924a0144b00ac01529a857db983",
      "3036cdb96e634ef28b17c5bb871ea65a",
      "cb16d1c2f05c4de09014db61c08a9daa",
      "e52201041bde4905a07d14b628bfe101",
      "3f0857f880374c7d98395ca2ee760ca5",
      "fa172dffd9794c809d10806b86ceb4e7",
      "61991795d73e4755ba8b1bccbcea777b",
      "5c6cb8fa13b8498d9771d0a9798ca580",
      "5e88e127ef2d44d49a0cf20240ada63a",
      "345f46825acc4c00a673d4110cd8a110",
      "78c1500a48404af89d6a387dafab6360",
      "daab6fa3f9b14579a7518dc7b0ee7cb0",
      "08014913ad1842b8966525b97aa3863d",
      "a08fa75284c84d2c8041dd7405087a64",
      "fdb2beeaa701467cbfd09c2b6994724f",
      "c6783c8821504eb9adf4375c5d9e6f99",
      "b31d4c1a591c4b318cc30e35f3282ed8",
      "956e114df8a3476f8ea3fb1c476b1829",
      "38ba53613cd043abbf6ab351715f9521",
      "b6614be980394c0faae2491d4b96bc0c",
      "08ab007cc61148339b76b867c6a236a1",
      "4dfbc76db03346628988e6193e51c4e7",
      "1049270c708a4fd89cdf9c08e6ea2c33",
      "45abe39d1d4e455086ebb5b126e5b8fb",
      "f72d6537d35a4c8a885545d704400c2e",
      "f3fc03a6035442d88479b88ef3a73fb4",
      "66b5574129824ea8ae71ead78f4b0933",
      "f392f2b996c24a31879c5ac3708016e6",
      "76abe31d7c264908872057638acabb4b",
      "3c4b0eda9bbe4d6bb3cdda2278d00c3a",
      "1c362f620cbe467c8ab57be80043f30a",
      "f1ff6e5bc8834c338d40e8834978d831",
      "fb625c8d0b114608b204b4630283aad4",
      "2e1b54a3d618415db1a129c4e0e333b8",
      "1be30aede60f4e91be71909b19de461c",
      "96ba7588f79f471ebc3575ce07ad6190",
      "0b6d008ea0234fddbe6cf6f0a5740180",
      "f292c02ce0a04877b42fdb7024e69324",
      "4bfa7e7829234b7c8446b46a512b48f4",
      "9d7cecb6b7fd4831b326f18ea4f1281a",
      "1b9138401351402fa970e0b93c820bb8",
      "0f2a0e24f88b4db6a2cfd89149d3bc3a",
      "0eb5392e5b5149ea8aa0ff5f75a92148",
      "e28016b4850146cbbde4e4c69da8ef9d",
      "4805b6871dc64dccb90cca74acad9ce3",
      "61b1557fc01e4902bae0a8bc3b47ef8c",
      "f8e0f9c880f543589329fbd5c6f3359a",
      "317a1e4185934b49ab9103f8f91f9a58",
      "8c17eb41018748e3a05f05a7372105f5",
      "102953290d89495ea61e5fbc36156af1",
      "3f8b0a43186e460ea3f6e95c3ba5e651",
      "9fca9687cf7c44ecb3442392ec91d54b",
      "2af8b0a42c5c4189865c1b712667c033",
      "47d75b13c35f47c9a40df43572d484f1",
      "d73ade1496b94e51819d9b17c11d0b8d",
      "5e5bfc3a068a42e49db09132155c4196",
      "44283e575cf74c38b224297915790b95",
      "c289dde7aff64025869b87b9ed556bb6",
      "27623e68fc9644b681956ca23dc08d96",
      "3554c5b7504c4c46ba1dbe1a62893d73",
      "b371f7d56c1d4a2687b0fe76b1ef1c28",
      "cebad31ac5eb47cd8f45cac8f7df7bda",
      "00414494cf52448799223b530a5ba2e4",
      "183e8ad8d6e44061a14bad64a5360cad",
      "99dfeb59c8014aa9928a9b8a1a363319",
      "77320a79e3fc4bd09e653e3f14c6aa6d",
      "85f04396457e4bf0ab7ca997481f4d1b",
      "1e77d7816c134091af39df85326fd6eb",
      "3ef3c753c4374cbabd1f2686ab337c7d",
      "72bc8ccd11a2435d9012e17b65cac293",
      "d9c1a45a27fb471eb8f7155ae7fd4e05",
      "b0d7c1241d504c478196d63d1945185e",
      "d8705e75e327493697dabfa401b1fed8",
      "9666f5072bdf4668bf991f2c18d9e063",
      "f2b037c915b94022bf0c8a811efc4e8d",
      "ff8a2065c3944782b63036f8abc14503",
      "13c72a3ac1084b5ab231b755247aa139",
      "aa6c2b799e27417196b3fff7754724a4",
      "53436c9a9bc1476f93855df0e8dba980",
      "50886de583ac42c88e4abb65e380ad4c",
      "52c12baf6bd44a548462bc89c7a08866",
      "d81e7a48bed54927b11ef9d0e8d3bf3a",
      "588770609156459b9408ba16283291ff",
      "a7f24c24ffdf495fade3968ec6d177c3",
      "e24d4f2443f04c44aa5042162e3e1f54",
      "8848b085ba9b4dcf87aa4bb4979aaa2b",
      "ef715753277d48c687128251de483f39",
      "53b9f7646ce342a39d783ce598aaacf1",
      "51157c9dd3084dc39a3382b8144fcd1b",
      "ad3534d0933141a5b1658b9565637c12",
      "dc68d9f7e93144698a64fc58b9231939",
      "2fefb528b26b44548da18fd13263ce39",
      "922f85f9cd994027865bc3e9761f0b2c",
      "6bfffa02b43f467b81d0d266536893d7",
      "d8c5c074759348a99dc3ebb894debd97",
      "eec4d9d4cf764236b6412101c3f8e50a",
      "db675712046542aca47730922aef94a8",
      "c2129045f98045ef8206fa70cdf9c2c9",
      "1e51a866a4804fb4bf86e89468bbc032",
      "902aa95c04fb468693a097f23f4bd105",
      "f18b2bec695745d5849e6b0fc3b85008",
      "980525d13eac4114a95350773181e37f",
      "02e38ea9844a462c9b696bc8ac09d791",
      "bd60031a11cc4d7b95e14ecf8ef56b26",
      "ca6775df9b2e4efe93ff9044b93cd7e9",
      "735dff620ea845e290189e2f41296705",
      "52f99f9b4ce9475ebab270db0341b17f",
      "ac78fe658afb4e78906d04c6a30f355a",
      "9d8f11ead271440cafba38179159a40d",
      "96be7dcc28f4435a9a4c05394a80c341",
      "922c619f20be4b34b50e8c5e84c73773",
      "e90d9c28908c41b7b1531f4e1a35c502",
      "c07f128465e7422e9acde08379a080d7",
      "f96e87b5d6e1492fb334cfaf4cf36851",
      "2508a4f1c0da4e979c21cec1582aa809",
      "617bf73da1af46e6ab666e9518816db8",
      "0fc34edeab2445abb18fb6031447ede6",
      "37c69898bddf40d185d991e4324f6455",
      "cb482ee6054b441db8bd5bfd346b7f1b",
      "418069b8ba3341d6866b171e3c32cb2d",
      "d0f5fcffc4c94d08a9a0fd9c994a8fc4",
      "c170bb87e82545f8b1b5fb9c81024af7",
      "8f67fa9afc7546129cd8be4599ba819f",
      "65c3061aad434ca2a3943dacd3da9e3f",
      "b3e5c43ec39a4dc6b68fce57a6c1e9c0",
      "93f8e6be81244887818c92029c97b507",
      "b469fe951e384e368f42ebf1495ca584",
      "08e50d7ad5844daab79dd8b881786af9",
      "674eadc463904c4996a0cc8a22be0049",
      "96a7612b084547168817b52cf3f8ce29",
      "31c1fa4718504d668cb1b968e199185d",
      "5403f41974204a5f9d09d65187e99eaf",
      "03ea101970724e12a9d1d44b3a96b379",
      "00e0a1bc9c4440b7bdc2a3d23a3b1e0d",
      "845b01f44bfb431b9e2b4b1f1a717f45",
      "76498d9b3eac4bb1bca49bca80ccb89c",
      "691954ffc1444fedbbc7420ac22759d0",
      "45b40a4cfa2444edacf4f36918d33f43",
      "04ba4c0bf22c4e77b8f7230916e9c436",
      "40d306376a9b46bbb9e018b19f37a5b0",
      "7bbafb44f8db48f8bc0ee5a3790f5a2d",
      "fbc3f78f9e34481fb184bdbbac8a225c",
      "35ad8940cedc4f179e6edf2c19b966e1",
      "dff2add83c7a456682a0f4d5326137d2",
      "b51fcb8155254addb92eb59ca97c72b8",
      "ad08171dd41e44c4b97e1c1c90b00b60",
      "d1ca64b9d4f5433698777e381c7f0dc4",
      "8e0d5284cdc64fafad687188d9ed5301",
      "3118980b3a0e474a93c6b0c3a46f1b7d",
      "8b3db173f73c419da33bb42ae561c7d5",
      "a7cf25c1423441c9a5f209c19fca6d92",
      "a4fd5b467c404f72ba0206ea2ebda1d6",
      "0d79dbca1ca64dc8b22c510aa1b3b0f3",
      "66a8a811580447568c8b0de08f804313",
      "92f796423be540a2aa9a92b701dd3aa1",
      "2793c2599a3042d38d17962f10a09c1c",
      "64a8477310384f7ea3d38bf445b9cb2b",
      "1c459fa3050742b285fb1d66bdfb6f0e",
      "b5d4c46ba4c749ceb4a1f3cc91cd9035",
      "726bd7b431a148e0b2f8e0f6edc36f8b",
      "ff493b6f7e95497481366ae7739a3368",
      "3881fd13bd01479c8f8c55c4fa5881d7",
      "4ff4bf744d8249e9b24feec9ea111d28",
      "f5307f775e094060957684a88b0b4d76",
      "778a7784b13d4fdab08a303ac9c51e4d",
      "d4960eb1f36142149d6635189c01f953",
      "78031d2b0d8e44ecb400e820c9662198",
      "10697e1e909545e4bcd483093087a165",
      "cae91aa2cfba4152a68879f9376f0009",
      "90b18c2cb9794262a53316f55498e452",
      "f50a20bfede84301ae33701351e34682",
      "210a8eaa998d4a46bade53be570281f0",
      "ee3eeaeb183646fd8e59ef5691a11866",
      "d03c83441d394ae4908a01d304dca5d3",
      "d6d2afca7cea4577837bce6e28055c11",
      "5a181b6b8e7b4ee9821adb403ee7eabe",
      "057f7cd95d6948f9bb20019e93088ed7"
     ]
    },
    "id": "qPCT40D0J90I",
    "outputId": "0dfe38e4-df2f-4fd2-90c9-c87e9966e55b"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtejaswiniksssn\u001b[0m (\u001b[33mtejaswiniksssn-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250414_081543-y9ab2si9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/y9ab2si9' target=\"_blank\">head_only</a></strong> to <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/y9ab2si9' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/y9ab2si9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0      | train\n",
      "1 | backbone  | ResNet           | 25.6 M | train\n",
      "-------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.268   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62e9ba804a540f0b1dd04ec0d572fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db2ec5887ce499a89f5550e5e561a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8ffc8bb36f4a129939176a0960990e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4328c00841df42e5828e4df081bd9bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a927e924a0144b00ac01529a857db983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daab6fa3f9b14579a7518dc7b0ee7cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1049270c708a4fd89cdf9c08e6ea2c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆▆█</td></tr><tr><td>train_acc</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▃▅▆▆█</td></tr><tr><td>val_acc</td><td>▁▅▆▇█</td></tr><tr><td>val_loss</td><td>█▆▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_acc</td><td>0.48438</td></tr><tr><td>train_loss</td><td>1.76896</td></tr><tr><td>trainer/global_step</td><td>64</td></tr><tr><td>val_acc</td><td>0.57</td></tr><tr><td>val_loss</td><td>1.62048</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">head_only</strong> at: <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/y9ab2si9' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/y9ab2si9</a><br> View project at: <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_081543-y9ab2si9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250414_081653-cxm0coun</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/cxm0coun' target=\"_blank\">partial</a></strong> to <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/cxm0coun' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/cxm0coun</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0      | train\n",
      "1 | backbone  | ResNet           | 25.6 M | train\n",
      "-------------------------------------------------------\n",
      "25.3 M    Trainable params\n",
      "225 K     Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.268   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45abe39d1d4e455086ebb5b126e5b8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be30aede60f4e91be71909b19de461c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b1557fc01e4902bae0a8bc3b47ef8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44283e575cf74c38b224297915790b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e77d7816c134091af39df85326fd6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53436c9a9bc1476f93855df0e8dba980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3534d0933141a5b1658b9565637c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆▆█</td></tr><tr><td>train_acc</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▃▅▆▆█</td></tr><tr><td>val_acc</td><td>▁▆▇██</td></tr><tr><td>val_loss</td><td>█▅▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_acc</td><td>1</td></tr><tr><td>train_loss</td><td>0.16762</td></tr><tr><td>trainer/global_step</td><td>64</td></tr><tr><td>val_acc</td><td>0.67</td></tr><tr><td>val_loss</td><td>0.95374</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">partial</strong> at: <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/cxm0coun' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/cxm0coun</a><br> View project at: <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_081653-cxm0coun/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250414_081815-rej7sk5w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/rej7sk5w' target=\"_blank\">last_block</a></strong> to <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/rej7sk5w' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/rej7sk5w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0      | train\n",
      "1 | backbone  | ResNet           | 25.6 M | train\n",
      "-------------------------------------------------------\n",
      "17.0 M    Trainable params\n",
      "8.5 M     Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.268   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18b2bec695745d5849e6b0fc3b85008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90d9c28908c41b7b1531f4e1a35c502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f67fa9afc7546129cd8be4599ba819f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e0a1bc9c4440b7bdc2a3d23a3b1e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51fcb8155254addb92eb59ca97c72b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2793c2599a3042d38d17962f10a09c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78031d2b0d8e44ecb400e820c9662198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆▆█</td></tr><tr><td>train_acc</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▃▅▆▆█</td></tr><tr><td>val_acc</td><td>▁▅▆██</td></tr><tr><td>val_loss</td><td>█▅▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_acc</td><td>0.96875</td></tr><tr><td>train_loss</td><td>0.35129</td></tr><tr><td>trainer/global_step</td><td>64</td></tr><tr><td>val_acc</td><td>0.71</td></tr><tr><td>val_loss</td><td>0.85683</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">last_block</strong> at: <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/rej7sk5w' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/rej7sk5w</a><br> View project at: <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_081815-rej7sk5w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Extract dataset if needed\n",
    "zip_file = \"/content/drive/MyDrive/nature_12K.zip\"\n",
    "extracted_dir = \"/content/inaturalist_12K/train\"\n",
    "\n",
    "if not os.path.exists(extracted_dir):\n",
    "    !cp \"{zip_file}\" .\n",
    "    !unzip -qq nature_12K.zip\n",
    "    !rm nature_12K.zip\n",
    "\n",
    "# Helper to create a class-balanced subset\n",
    "def sample_balanced_data(dataset, per_class_limit = 100):\n",
    "    class_to_indices = {label: [] for label in range(len(dataset.classes))}\n",
    "    for idx, (_, cls) in enumerate(dataset.samples):\n",
    "        class_to_indices[cls].append(idx)\n",
    "    selected = []\n",
    "    for indices in class_to_indices.values():\n",
    "        selected.extend(random.sample(indices, min(per_class_limit, len(indices))))\n",
    "    return Subset(dataset, selected)\n",
    "\n",
    "# Fine-tuning class using PyTorch Lightning\n",
    "class TransferLearner(pl.LightningModule):\n",
    "    def __init__(self, mode=\"head_only\", start_unfreeze_layer=5):\n",
    "        super().__init__()\n",
    "        self.strategy = mode\n",
    "        self.lr = 1e-4\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Use ResNet50 pretrained model\n",
    "        net = torchvision.models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
    "        original_fc = net.fc  # Retain the original 1000-class FC layer\n",
    "\n",
    "        # Replace fc with extended 10-class output layer\n",
    "        net.fc = nn.Sequential(\n",
    "            original_fc,         # 1000-class output\n",
    "            nn.ReLU(),           # Optional non-linearity\n",
    "            nn.Linear(1000, 10)  # Final output layer for iNaturalist\n",
    "        )\n",
    "\n",
    "        self.backbone = net\n",
    "        self.configure_finetune(start_unfreeze_layer)\n",
    "\n",
    "    def configure_finetune(self, unfreeze_from):\n",
    "        \"\"\"Freeze layers based on the selected strategy.\"\"\"\n",
    "        if self.strategy == \"head_only\":\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.backbone.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        elif self.strategy == \"partial\":\n",
    "            child_count = 0\n",
    "            for child in self.backbone.children():\n",
    "                child_count += 1\n",
    "                requires_grad = child_count > unfreeze_from\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = requires_grad\n",
    "\n",
    "        elif self.strategy == \"last_block\":\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                if \"layer4\" in name or \"fc\" in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.criterion(preds, y)\n",
    "        acc = (preds.argmax(1) == y).float().mean()\n",
    "        self.log_dict({\"train_loss\": loss, \"train_acc\": acc}, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.criterion(preds, y)\n",
    "        acc = (preds.argmax(1) == y).float().mean()\n",
    "        self.log_dict({\"val_loss\": loss, \"val_acc\": acc}, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        return torch.optim.Adam(trainable_params, lr=self.lr)\n",
    "\n",
    "# Fine-tuning class using PyTorch Lightning\n",
    "class TransferLearner(pl.LightningModule):\n",
    "    def __init__(self, mode=\"head_only\", start_unfreeze_layer=5):\n",
    "        super().__init__()\n",
    "        self.strategy = mode\n",
    "        self.lr = 1e-4\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Use ResNet50 pretrained model\n",
    "        net = torchvision.models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
    "        original_fc = net.fc  # Retain the original 1000-class FC layer\n",
    "\n",
    "        # Replace fc with extended 10-class output layer\n",
    "        net.fc = nn.Sequential(\n",
    "            original_fc,         # 1000-class output\n",
    "            nn.ReLU(),           # Optional non-linearity\n",
    "            nn.Linear(1000, 10)  # Final output layer for iNaturalist\n",
    "        )\n",
    "\n",
    "        self.backbone = net\n",
    "        self.configure_finetune(start_unfreeze_layer)\n",
    "\n",
    "    def configure_finetune(self, unfreeze_from):\n",
    "        \"\"\"Freeze layers based on the selected strategy.\"\"\"\n",
    "        if self.strategy == \"head_only\":\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.backbone.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        elif self.strategy == \"partial\":\n",
    "            child_count = 0\n",
    "            for child in self.backbone.children():\n",
    "                child_count += 1\n",
    "                requires_grad = child_count > unfreeze_from\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = requires_grad\n",
    "\n",
    "        elif self.strategy == \"last_block\":\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                if \"layer4\" in name or \"fc\" in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.criterion(preds, y)\n",
    "        acc = (preds.argmax(1) == y).float().mean()\n",
    "        self.log_dict({\"train_loss\": loss, \"train_acc\": acc}, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.criterion(preds, y)\n",
    "        acc = (preds.argmax(1) == y).float().mean()\n",
    "        self.log_dict({\"val_loss\": loss, \"val_acc\": acc}, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        return torch.optim.Adam(trainable_params, lr=self.lr)\n",
    "\n",
    "# Custom DataModule with image resizing and optional cropping\n",
    "class INaturalistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.path = \"/content/inaturalist_12K/train\"\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        preprocess = T.Compose([\n",
    "            T.Resize((224, 224)),  # Ensures ImageNet compatibility\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.4712, 0.4600, 0.3896], std=[0.2406, 0.2301, 0.2406])\n",
    "        ])\n",
    "        base_data = ImageFolder(self.path, transform=preprocess)\n",
    "        subset = sample_balanced_data(base_data)\n",
    "        val_split = int(0.2 * len(subset))\n",
    "        train_split = len(subset) - val_split\n",
    "        self.train_set, self.val_set = torch.utils.data.random_split(subset, [train_split, val_split])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Launch a single experiment with W&B logging\n",
    "def launch(mode_name):\n",
    "    wandb_logger = WandbLogger(project=\"inat-resnet-ft\", name=mode_name)\n",
    "    learner = TransferLearner(mode=mode_name)\n",
    "    data = INaturalistDataModule()\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5,\n",
    "        logger=wandb_logger,\n",
    "        accelerator=\"auto\",\n",
    "        callbacks=[EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=2)],\n",
    "    )\n",
    "    trainer.fit(learner, datamodule=data)\n",
    "    wandb.finish()\n",
    "\n",
    "# Try different strategies\n",
    "for strategy in [\"head_only\", \"partial\", \"last_block\"]:\n",
    "    launch(strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XO-5rNxkDFP"
   },
   "source": [
    "**Question 03**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d4789b93e74247b6a6e56932f0e6661d",
      "0836c85c615f45f090927b0036e8a59d",
      "0f2e819ecb924092ba04309bc895dd70",
      "f39a7f04479d4fa28bd6cbcd1bdf8116",
      "5d15cd270a16472997673b72d21ba56c",
      "2f210472409c45b58d7a31f5e86c9763",
      "2a68eaf9caad4315b2f813c75b079a53",
      "c6d2213f0dc647c3bae11f05ed3c255e",
      "79eefcaf1faa4b3aaee1bf77d063ac1d",
      "2468c523a51a4342a50727fe825a575d",
      "67bf0373ecc24989a6945c24ef4e9b22",
      "4b653e00cc5a442f9d2d3b70b4ec78f5",
      "10c7c1d1893a4c4fa25b10683777789f",
      "0a90b04f4f454e578c5aaf72d7511433",
      "b265ce9250e5453cbabb3055073e5477",
      "e3e2ad2d71c94b83aebaca7ab6862a19",
      "d86e0adf9fcb4378a3b5f4b5ac81cdc8",
      "e554c49a2a7149a690099526a3f5c9ad",
      "464ce87d914b4d15b38fc4b7ab30d112",
      "5618c022ae744ddd98ff0d78144fc136",
      "76920f64a6f74ec3b18849979392c0b9",
      "3eef3d9b113e40d29ca79d2e54d0df8d"
     ]
    },
    "id": "mAd0ho__Y_nr",
    "outputId": "75ca8842-290d-4a87-90d3-c4cab4ef5836"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250414_085652-14szszan</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/14szszan' target=\"_blank\">last_block</a></strong> to <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/14szszan' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/14szszan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0      | train\n",
      "1 | backbone  | ResNet           | 25.6 M | train\n",
      "-------------------------------------------------------\n",
      "17.0 M    Trainable params\n",
      "8.5 M     Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.268   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4789b93e74247b6a6e56932f0e6661d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b653e00cc5a442f9d2d3b70b4ec78f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.840499997138977     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6410735845565796     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.840499997138977    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6410735845565796    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▄▄▄▅▅▅▇▇▇█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▇▅▇▇▇█▇██████</td></tr><tr><td>train_loss</td><td>█▆▃▃▂▂▂▁▂▂▁▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>test_acc</td><td>0.8405</td></tr><tr><td>test_loss</td><td>0.64107</td></tr><tr><td>train_acc</td><td>1</td></tr><tr><td>train_loss</td><td>0.01008</td></tr><tr><td>trainer/global_step</td><td>785</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">last_block</strong> at: <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/14szszan' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft/runs/14szszan</a><br> View project at: <a href='https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft' target=\"_blank\">https://wandb.ai/tejaswiniksssn-indian-institute-of-technology-madras/inat-resnet-ft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_085652-14szszan/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Extract dataset if needed\n",
    "zip_file = \"/content/drive/MyDrive/nature_12K.zip\"\n",
    "extracted_dir = \"/content/inaturalist_12K/train\"\n",
    "\n",
    "if not os.path.exists(extracted_dir):\n",
    "    !cp \"{zip_file}\" .\n",
    "    !unzip -qq nature_12K.zip\n",
    "    !rm nature_12K.zip\n",
    "\n",
    "# Fine-tuning class using PyTorch Lightning\n",
    "class TransferLearner(pl.LightningModule):\n",
    "    def __init__(self, mode=\"last_block\"):\n",
    "        super().__init__()\n",
    "        self.strategy = mode\n",
    "        self.lr = 1e-4\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Use ResNet50 pretrained model\n",
    "        net = torchvision.models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
    "        original_fc = net.fc  # Retain the original 1000-class FC layer\n",
    "\n",
    "        # Replace fc with extended 10-class output layer\n",
    "        net.fc = nn.Sequential(\n",
    "            original_fc,         # 1000-class output\n",
    "            nn.ReLU(),           # Optional non-linearity\n",
    "            nn.Linear(1000, 10)  # Final output layer for iNaturalist\n",
    "        )\n",
    "\n",
    "        self.backbone = net\n",
    "        self.configure_finetune()\n",
    "\n",
    "    def configure_finetune(self):\n",
    "        \"\"\"Freeze layers based on the selected strategy.\"\"\"\n",
    "        if self.strategy == \"last_block\":\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                if \"layer4\" in name or \"fc\" in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.criterion(preds, y)\n",
    "        acc = (preds.argmax(1) == y).float().mean()\n",
    "        self.log_dict({\"train_loss\": loss, \"train_acc\": acc}, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.criterion(preds, y)\n",
    "        acc = (preds.argmax(1) == y).float().mean()\n",
    "        self.log_dict({\"test_loss\": loss, \"test_acc\": acc}, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        return torch.optim.Adam(trainable_params, lr=self.lr)\n",
    "\n",
    "# Custom DataModule for iNaturalist with resizing and normalization\n",
    "class INaturalistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.path = \"/content/inaturalist_12K\"\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        preprocess = T.Compose([\n",
    "            T.Resize((224, 224)),  # Ensures ImageNet compatibility\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.4712, 0.4600, 0.3896], std=[0.2406, 0.2301, 0.2406])\n",
    "        ])\n",
    "        # Load the entire dataset\n",
    "        train_data = ImageFolder(os.path.join(self.path, \"train\"), transform=preprocess)\n",
    "        test_data = ImageFolder(os.path.join(self.path, \"val\"), transform=preprocess)\n",
    "\n",
    "        # Set up datasets\n",
    "        self.train_set = train_data\n",
    "        self.test_set = test_data\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Launch a single experiment with W&B logging for 'last_block' fine-tuning strategy\n",
    "def launch():\n",
    "    mode_name = \"last_block\"\n",
    "    wandb_logger = WandbLogger(project=\"inat-resnet-ft\", name=mode_name)\n",
    "    learner = TransferLearner(mode=mode_name)\n",
    "    data = INaturalistDataModule()\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5,\n",
    "        logger=wandb_logger,\n",
    "        accelerator=\"auto\",\n",
    "    )\n",
    "    trainer.fit(learner, datamodule=data)\n",
    "    trainer.test(learner, datamodule=data)  # Run the test set after training\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the experiment with last block fine-tuning\n",
    "launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzvXXg9hkAi8"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "# Path to current notebook\n",
    "notebook_path = '/content/drive/MyDrive/PartB.ipynb'  # Change this!\n",
    "\n",
    "# Load the notebook\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Remove metadata.widgets\n",
    "if 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "    print(\"Removed 'metadata.widgets'\")\n",
    "else:\n",
    "    print(\"No 'metadata.widgets' found\")\n",
    "\n",
    "# Save back to the same file (or change filename)\n",
    "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(nb, f)\n",
    "    print(\"Saved cleaned notebook\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
